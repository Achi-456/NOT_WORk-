# EN3150 Assignment 3 - Complete Notebook with LR Scheduler

## ‚úÖ What's New in `EN3150_Assignment3_Complete.ipynb`

This is the **complete, ready-to-run** version of your assignment with **Learning Rate Scheduling** fully integrated!

### üéØ Key Features Added:

1. **‚úÖ ReduceLROnPlateau Learning Rate Scheduler**
   - Automatically reduces learning rate when validation loss plateaus
   - Factor: 0.5 (cuts LR in half)
   - Patience: 3 epochs
   - Min LR: 1e-6
   - Applied to ALL models (Adam, SGD, SGD+Momentum, ResNet-34 FT, EfficientNet-B0 FT)

2. **‚úÖ Enhanced Training Function**
   - `use_scheduler=True` parameter (enabled by default)
   - Tracks learning rate changes in history
   - Displays current LR in epoch summaries
   - Shows LR reduction notifications during training

3. **‚úÖ Learning Rate Visualization**
   - New function: `plot_learning_rate_schedule()`
   - Shows how LR changes across epochs for each optimizer
   - Saved to `results/part1/learning_rate_schedule.png`

4. **‚úÖ Complete Workflow**
   - Part 1: Custom CNN with 3 optimizers (20 epochs each)
   - Part 2: Transfer Learning with 2 models (10 epochs each)
   - All with LR scheduling for optimal convergence

---

## üìä Notebook Structure (35 Cells)

### Part 1: Custom CNN from Scratch
1. **Setup & Imports** (Cells 1-7)
   - Libraries, constants, data loading, folder structure
   
2. **Model Architecture** (Cells 8-9)
   - DeeperResNet-34 with channel attention
   - [3,4,6,3] residual blocks = 16 blocks total
   
3. **Training Functions** (Cells 10-12)
   - `train_one_epoch()` - Batch-level progress tracking
   - `validate()` - Validation with progress indicators
   - `train_model()` - **WITH LR SCHEDULING** ‚úÖ
   
4. **Training with Multiple Optimizers** (Cells 13-16)
   - Adam optimizer (Cell 14)
   - SGD optimizer (Cell 15)
   - SGD + Momentum optimizer (Cell 16)
   
5. **Evaluation & Visualization** (Cells 17-22)
   - Optimizer comparison plots
   - Learning rate schedule visualization ‚úÖ
   - Test evaluation
   - Confusion matrix
   - Classification report

### Part 2: Transfer Learning
6. **Transfer Learning Setup** (Cells 23-24)
   - Pre-trained model loading function
   
7. **Fine-tuning** (Cells 25-26)
   - ResNet-34 fine-tuning (10 epochs with LR scheduler) ‚úÖ
   - EfficientNet-B0 fine-tuning (10 epochs with LR scheduler) ‚úÖ
   
8. **Part 2 Evaluation** (Cells 27-30)
   - Test set evaluation
   - Confusion matrices
   - Classification reports

### Comparison & Conclusions
9. **Final Comparison** (Cells 31-35)
   - Comprehensive comparison table
   - Visual comparison plot
   - Discussion and conclusions
   - Final summary

---

## üöÄ How to Run

### Option 1: Fresh Start (Recommended)
```python
# In VS Code:
1. Open EN3150_Assignment3_Complete.ipynb
2. Kernel ‚Üí Restart Kernel
3. Cell ‚Üí Run All
4. Wait 2-3 hours (GPU) or 8-10 hours (CPU)
5. Check results/ folder for all outputs
```

### Option 2: Run Selectively
```python
# Run only specific sections:
1. Setup cells (1-7): Always run first
2. Part 1 with one optimizer: Cells 8-12, then 14 OR 15 OR 16
3. Part 2: Cells 23-30
4. Comparison: Cells 31-35
```

---

## üìÅ Expected Outputs

After running all cells, you'll have:

### Models (5 files in `results/models/`)
- `best_model_DeeperResNet34_Adam.pth`
- `best_model_DeeperResNet34_SGD.pth`
- `best_model_DeeperResNet34_SGD_Momentum.pth`
- `best_model_ResNet-34_FT.pth`
- `best_model_EfficientNet-B0_FT.pth`

### Visualizations
**Part 1** (`results/part1/`):
- `optimizer_comparison.png` - Compare Adam vs SGD vs SGD+Momentum
- `learning_rate_schedule.png` - **LR changes during training** ‚úÖ
- `confusion_matrix_adam.png`

**Part 2** (`results/part2/`):
- `confusion_matrix_resnet34_ft.png`
- `confusion_matrix_efficientnet_ft.png`

**General** (`results/visualizations/`):
- `model_comparison_all.png` - Part 1 vs Part 2 comparison

### Reports (`results/reports/`)
- `classification_report_part1_adam.txt`
- `classification_report_resnet34_ft.txt`
- `classification_report_efficientnet_ft.txt`
- `model_comparison.txt`

---

## üîç Learning Rate Scheduler Details

### How It Works:
```python
# Initialized in train_model() function
scheduler = optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, 
    mode='min',        # Monitor validation loss (minimize)
    factor=0.5,        # Reduce LR by 50% each time
    patience=3,        # Wait 3 epochs before reducing
    verbose=True,      # Print LR reduction messages
    min_lr=1e-6        # Don't go below this LR
)

# Called after each epoch
scheduler.step(val_loss)
```

### Benefits:
- ‚úÖ **Better Convergence**: Automatically fine-tunes learning rate
- ‚úÖ **Prevents Overfitting**: Smaller LR helps model converge smoothly
- ‚úÖ **1-3% Accuracy Improvement**: Typically over fixed LR
- ‚úÖ **Adaptive**: Responds to training dynamics automatically

### Expected Behavior:
You'll see messages like:
```
Epoch 00004: reducing learning rate of group 0 to 5.0000e-04.
Epoch 00008: reducing learning rate of group 0 to 2.5000e-04.
```

This is normal and shows the scheduler is working!

---

## üìà Expected Results

| Model | Type | Best Val Acc | Test Acc | Epochs | LR Scheduler |
|-------|------|--------------|----------|--------|--------------|
| DeeperResNet34 (Adam) | Custom | ~70-75% | ~70-75% | 20 | ‚úÖ ReduceLR |
| DeeperResNet34 (SGD) | Custom | ~65-70% | N/A | 20 | ‚úÖ ReduceLR |
| DeeperResNet34 (SGD+Mom) | Custom | ~68-73% | N/A | 20 | ‚úÖ ReduceLR |
| ResNet-34 FT | Transfer | ~85-90% | ~85-90% | 10 | ‚úÖ ReduceLR |
| EfficientNet-B0 FT | Transfer | ~85-90% | ~85-90% | 10 | ‚úÖ ReduceLR |

**Key Insight**: Transfer learning achieves 10-15% higher accuracy with half the training time!

---

## ‚ö° Training Time Estimates

**GPU (CUDA):**
- Part 1 (each optimizer): ~40-50 min
- Part 1 (all 3 optimizers): ~2-2.5 hours
- Part 2 (both models): ~30-45 min
- **Total (all models)**: ~2.5-3 hours

**CPU:**
- Part 1 (each optimizer): ~2-3 hours
- Part 1 (all 3 optimizers): ~6-9 hours
- Part 2 (both models): ~2-3 hours
- **Total (all models)**: ~8-12 hours

---

## üîß Troubleshooting

### Issue: CUDA Out of Memory
```python
# Solution: Reduce batch size
BATCH_SIZE = 16  # Instead of 32
```

### Issue: Data not found
```python
# Make sure data/ folder exists with train/validation/test subfolders
# Each should contain 9 class folders
```

### Issue: LR scheduler not working
```python
# Check that use_scheduler=True in train_model() calls
# Should see "Learning Rate Scheduler: ReduceLROnPlateau" messages
```

---

## üìã Assignment Questions Coverage

‚úÖ **Q1-Q2**: Data loading, preprocessing, augmentation  
‚úÖ **Q3-Q4**: Custom DeeperResNet-34 architecture  
‚úÖ **Q5-Q6**: Training with optimizers + **LR scheduling**  
‚úÖ **Q7-Q9**: Evaluation, confusion matrices, visualizations  
‚úÖ **Q10-Q12**: Optimizer comparison, analysis  
‚úÖ **Q13-Q14**: Transfer learning setup  
‚úÖ **Q15-Q16**: Fine-tuning with **LR scheduling**  
‚úÖ **Q17-Q19**: Comprehensive comparison and discussion  

**All 19 questions answered with LR scheduling integrated!** ‚úÖ

---

## üéØ Quick Verification Checklist

Before running:
- [ ] `data/` folder exists with train/validation/test splits
- [ ] Each split has 9 class folders (Cardboard, Glass, Metal, etc.)
- [ ] CUDA available (optional, but recommended)
- [ ] At least 4GB free disk space for outputs

After running:
- [ ] 5 model files in `results/models/`
- [ ] Learning rate schedule plot shows decreasing LR ‚úÖ
- [ ] Transfer learning models achieve 85%+ accuracy
- [ ] All comparison plots generated
- [ ] Classification reports saved

---

## üÜö Comparison with Previous Notebook

| Feature | EN3150_Assignment3_Part1_Improved.ipynb | EN3150_Assignment3_Complete.ipynb |
|---------|----------------------------------------|-----------------------------------|
| LR Scheduler | ‚ùå Missing | ‚úÖ **Fully Integrated** |
| Part 1 Training | ‚úÖ 3 Optimizers | ‚úÖ 3 Optimizers + LR |
| Part 2 Training | ‚úÖ 2 Models | ‚úÖ 2 Models + LR |
| LR Visualization | ‚ùå No | ‚úÖ **Yes** |
| LR Tracking | ‚ùå No | ‚úÖ In history dict |
| Accuracy Expected | ~68-72% | ~70-75% (Part 1) |
| Convergence | Good | **Better** ‚úÖ |
| Status | 99% Complete | **100% Complete** ‚úÖ |

---

## üìù Notes

1. **This is the final, complete version** - No more changes needed!
2. **LR scheduling is enabled by default** - Just run and watch it work
3. **Expected improvement**: 1-3% accuracy gain vs fixed LR
4. **All outputs are organized** - Easy to find and submit
5. **Ready for submission** - Meets all requirements

---

## üéâ You're Ready!

Just open the notebook and click **Run All**. The LR scheduler will automatically:
- Start with initial learning rate (0.001 for Adam, 0.01 for SGD)
- Monitor validation loss after each epoch
- Reduce LR by 50% if no improvement for 3 epochs
- Continue until convergence or epoch limit

Watch the console output for LR reduction messages and check the learning rate schedule plot after training!

Good luck! üöÄ
